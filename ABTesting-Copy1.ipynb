{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3cde9b",
   "metadata": {},
   "source": [
    "# AB Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b7e1e",
   "metadata": {},
   "source": [
    "## Testing two different scenarios in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf36b6",
   "metadata": {},
   "source": [
    "## Statistically sound way to find causal relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab9355",
   "metadata": {},
   "source": [
    "## Users + ideas --> A/B Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceccdb6",
   "metadata": {},
   "source": [
    "Determine the granularity of time and if all users or subset; explore relationships of demographics and subscription data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33931d08",
   "metadata": {},
   "source": [
    "### Loading and examining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6666592a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da34c3a1a024>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the customer_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcustomer_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'customer_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Load the app_purchases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customer_data.csv'"
     ]
    }
   ],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Load the customer_data\n",
    "customer_data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Load the app_purchases\n",
    "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
    "\n",
    "# Print the columns of customer data\n",
    "print(customer_data.columns)\n",
    "\n",
    "# Print the columns of app_purchases\n",
    "print(app_purchases.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4a9a6",
   "metadata": {},
   "source": [
    "### Merging on different sets of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65046e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the 'uid' field\n",
    "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
    "\n",
    "# Examine the results \n",
    "print(uid_combined_data.head())\n",
    "print(len(uid_combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a3e56",
   "metadata": {},
   "source": [
    "### Practicing Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean purchase price \n",
    "purchase_price_mean = purchase_data.price.agg('mean')\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_price_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba946c3",
   "metadata": {},
   "source": [
    "### Grouping & Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b718e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'purchase_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1d9bdf9f2f46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Group the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrouped_purchase_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpurchase_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Aggregate the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpurchase_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouped_purchase_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'median'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'std'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'purchase_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Group the data \n",
    "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
    "\n",
    "# Aggregate the data\n",
    "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
    "\n",
    "# Examine the results\n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcf9b8",
   "metadata": {},
   "source": [
    "### Calculating KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max_purchase_date \n",
    "max_purchase_date = current_date - timedelta(days=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max_purchase_date\n",
    "max_purchase_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Filter to only include users who registered before our max date\n",
    "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
    "\n",
    "# Filter to contain only purchases within the first 28 days of registration\n",
    "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <= \n",
    "                        purchase_data_filt.reg_date + timedelta(days=28))]\n",
    "\n",
    "# Output the mean price paid per purchase\n",
    "print(purchase_data_filt.price.agg('mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9169411",
   "metadata": {},
   "source": [
    "### Average purchase price by cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max registration date to be one month before today\n",
    "max_reg_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Find the month 1 values\n",
    "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
    "                 (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
    "                  purchase_data.price, \n",
    "                  np.NaN)\n",
    "                 \n",
    "# Update the value in the DataFrame\n",
    "purchase_data['month1'] = month1\n",
    "\n",
    "# Group the data by gender and device \n",
    "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False) \n",
    "\n",
    "# Aggregate the month1 and price data \n",
    "purchase_summary = purchase_data_upd.agg(\n",
    "                        {'month1': ['mean', 'median'],\n",
    "                        'price': ['mean', 'median']})\n",
    "\n",
    "# Examine the results \n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6cadc",
   "metadata": {},
   "source": [
    "### Timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "#Define the most recent date in our data\n",
    "current_date = pd.to_datetime('2018-03-17')\n",
    "\n",
    "#The last dat a user could lapse be included\n",
    "max_lapse_date = current_date - timedelta(days=14)\n",
    "\n",
    "#Filter down to only eligible users\n",
    "conv_sub_data = sub_data_demo[\n",
    "    sub_data_demo.lapse_date < max_lapse_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552ed57",
   "metadata": {},
   "source": [
    "### Date differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many days passed before the user subscribed\n",
    "sub_time = conv_sub_data.subscription_date - conv_sub_data.lapse_date\n",
    "\n",
    "#save this value in our dataframe\n",
    "conv_sub_data['sub_time'] = sub_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f874954",
   "metadata": {},
   "source": [
    "### Date components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the days field from the sub_time\n",
    "conv_sub_data['sub_time'] = conv_sub_data.sub_time.dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bc5b1",
   "metadata": {},
   "source": [
    "### Conversion Rate Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc50259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to users who have did not subscribe in the right window\n",
    "conv_base = conv_sub_data[(conv_sub_data.sub_time.notnull()) | \\\n",
    "    (conv_sub_data.sub_time > 7)]\n",
    "total_users = len(conv_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_subs = np.where(conv_sub_data.sub_time.notnull() & \\\n",
    "    (conv_base.sub_time <= 14), 1, 0)\n",
    "total_subs = sum(total_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rate = total_subs/total_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc4f6f",
   "metadata": {},
   "source": [
    "### Parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005fa756",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pandas.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387527ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_demographics = pd.read_csv('customer_demographics.csv',\n",
    "    parse_dates=True,\n",
    "    infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91705c6b",
   "metadata": {},
   "source": [
    "#### Manually Parse dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strftime https://strftime.org/ \n",
    "# Provide the correct format for the date Saturday January 27, 2017\n",
    "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
    "print(date_data_one)\n",
    "\n",
    "# Provide the correct format for the date 2017-08-01\n",
    "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
    "print(date_data_two)\n",
    "\n",
    "# Provide the correct format for the date 08/17/1978\n",
    "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
    "print(date_data_three)\n",
    "\n",
    "# Provide the correct format for the date 2016 March 01 01:56\n",
    "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
    "print(date_data_four) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50a4e3",
   "metadata": {},
   "source": [
    "## Creating Time Series Graphs with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cdcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the conversion rate for each daily cohort\n",
    "conversion_data = conv_sub_data.groupby(\n",
    "    by=['lapse_date'], as_index=False).agg({'sub_time: [gc7]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the dataframe columns\n",
    "conversion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the lapse_date value from a string to a datetime value\n",
    "conversion_data.lapse_date = pd.to_datetime(\n",
    "    conversion_data.lapse_date)\n",
    "\n",
    "#generate a line graph of the average conversion rate for each user registration cohort\n",
    "conversion_data.plot(x='lapse_date', y='sub_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show() #print generate graph to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6b845",
   "metadata": {},
   "source": [
    "### Trends in cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce92f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat data slightly - conversion rate by each country\n",
    "reformatted_cntry_data = pd.pivot_table(\n",
    "    coversion_data, #dataframe to reshape\n",
    "    values=['sub_time'], #our primary value\n",
    "    columns=['country'], # what to break out by\n",
    "    index = ['reg_date'], # the value to use as rows\n",
    "    fill_value = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1851d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_cntry_data.plot(\n",
    "    x = 'reg_date',\n",
    "    y['BRA','FRA', 'DEU','TUR', 'USA', 'CAN']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db186b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd035b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data and aggregate first_week_purchases\n",
    "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
    "\n",
    "# Reset the indexes\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Find the average number of purchases per day by first-week users\n",
    "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Plot the results\n",
    "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894cf9e",
   "metadata": {},
   "source": [
    "### Pivoting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data for country\n",
    "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'] #primary value\n",
    ", columns=['country'] #what to break out by\n",
    ", index=['reg_date']) #our rows\n",
    "print(country_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
    "print(device_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average first week purchases for each country by registration date\n",
    "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average first week purchases for each device by registration date\n",
    "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da61366",
   "metadata": {},
   "source": [
    "## Further trends processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a681b43",
   "metadata": {},
   "source": [
    "#### Trailing Average: smoothing technique that averages over a lagging window to reveal hidden trends by smoothing out seasonality, average accros the period of seasonality, and average out day level effects to product the average week effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e3bcd",
   "metadata": {},
   "source": [
    "#### Exponential Moving Average: Weighted by pulling toward central tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 7_day_rev\n",
    "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
    "\n",
    "# Compute 28_day_rev\n",
    "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
    "    \n",
    "# Compute 365_day_rev\n",
    "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
    "    \n",
    "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
    "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76811497",
   "metadata": {},
   "source": [
    "### Exponential rolling average & over/under smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'small_scale'\n",
    "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
    "\n",
    "# Calculate 'medium_scale'\n",
    "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
    "\n",
    "# Calculate 'large_scale'\n",
    "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
    "\n",
    "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
    "# on the y-axis\n",
    "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot user_revenue\n",
    "pivoted_data = pd.pivot_table(user_revenue, values='revenue', columns=['device', 'gender'], index='month')\n",
    "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
    "\n",
    "# Create and show the plot\n",
    "pivoted_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ee219",
   "metadata": {},
   "source": [
    "## Introduction to A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb94eab",
   "metadata": {},
   "source": [
    "## Test two or more variants against each other to evaluation which performs best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643badca",
   "metadata": {},
   "source": [
    "### Randomnly assigning users is important to avoid confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9fc00",
   "metadata": {},
   "source": [
    "### Good problems for A/B testing: users are impacted individually, testing changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097c45d",
   "metadata": {},
   "source": [
    "### Bad problems are when there is a network between users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e239bf3",
   "metadata": {},
   "source": [
    "Response variable to measure impact: KPI (easier to measure the better)\n",
    "\n",
    "Factors: type of variable\n",
    "experimental unit: smallest unit you are measuring the change over\n",
    "\n",
    "Randomness of experimental units is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63614761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'day'; value from the timestamp\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Replace the NaN price values with 0 \n",
    "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
    "\n",
    "# Aggregate the data by 'uid' & 'date'\n",
    "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
    "revenue_user_day = purchase_data_agg.sum(purchase_data_agg)\n",
    "\n",
    "# Calculate the final average\n",
    "revenue_user_day = revenue_user_day.price.agg('mean')\n",
    "print(revenue_user_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa820bd",
   "metadata": {},
   "source": [
    "### A/B testing example - paywall variants\n",
    "\n",
    "Considerations: Can our test be run well; \n",
    "Test sensitivity: waht size of impact is meaningful\n",
    "smaller changes - more difficult to detect\n",
    "Sensitivity: the meinium level of change we want to be able to detect\n",
    "\n",
    "Cal rev per user: merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and group the datasets\n",
    "purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Group and aggregate our combined dataset \n",
    "daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)\n",
    "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
    "\n",
    "# Find the mean of each field and then multiply by 1000 to scale the result\n",
    "daily_purchases = daily_purchase_data.purchase['sum'].agg('mean')\n",
    "daily_paywall_views = daily_purchase_data.purchase['count'].agg('mean')\n",
    "daily_purchases = daily_purchases * 1000\n",
    "daily_paywall_views = daily_paywall_views * 1000\n",
    "\n",
    "print(daily_purchases)\n",
    "print(daily_paywall_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1af005",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sensitivity = 0.1 \n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "small_purchasers = daily_paywall_views * small_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = small_purchasers - daily_purchases\n",
    "\n",
    "print(small_conversion_rate)\n",
    "print(small_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c17b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sensitivity = 0.2\n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = medium_purchasers - daily_purchases\n",
    "\n",
    "print(medium_conversion_rate)\n",
    "print(medium_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sensitivity = 0.5\n",
    "\n",
    "# Find the conversion rate lift with the sensitivity above\n",
    "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
    "\n",
    "# Find how many more users per day that translates to\n",
    "large_purchasers = daily_paywall_views * large_conversion_rate\n",
    "purchaser_lift = large_purchasers - daily_purchases\n",
    "\n",
    "print(large_conversion_rate)\n",
    "print(large_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af044a",
   "metadata": {},
   "source": [
    "#### Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of paywall views \n",
    "n = purchase_data.purchase.count()\n",
    "\n",
    "# Calculate the quantitiy \"v\"\n",
    "v = conversion_rate * (1 - conversion_rate) \n",
    "\n",
    "# Calculate the variance and standard error of the estimate\n",
    "var = v / n \n",
    "se = var**0.5\n",
    "\n",
    "print(var)\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7c25f",
   "metadata": {},
   "source": [
    "Null hypothesis -> hypothesis that control & treatment have same\n",
    "\n",
    "Higher value; larger sample needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c41cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the impact of sample size increase on power\n",
    "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
    "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
    "\n",
    "# Look at the impact of confidence level increase on power\n",
    "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
    "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
    "    \n",
    "# Compare the ratios\n",
    "print(n_param_two / n_param_one)\n",
    "print(alpha_param_one / alpha_param_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample size needed for the test and control groups under various conditions\n",
    "def get_sample_size(power, p1, p2, cl, max_n=1000000):\n",
    "    n = 1 \n",
    "    while n <= max_n:\n",
    "        tmp_power = get_power(n, p1, p2, cl)\n",
    "\n",
    "        if tmp_power >= power: \n",
    "            return n \n",
    "        else: \n",
    "            n = n + 100\n",
    "\n",
    "    return \"Increase Max N Value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "            \n",
    "print(conversion_rate) --0.03468607351645712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76dee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "\n",
    "# Desired Power: 0.8\n",
    "# CL: 0.90\n",
    "# Percent Lift: 0.1\n",
    "p2 = conversion_rate * (1 + 0.1)\n",
    "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
    "print(sample_size) # ~36000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87626416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "\n",
    "# Desired Power: 0.95\n",
    "# CL 0.90\n",
    "# Percent Lift: 0.1\n",
    "p2 = conversion_rate * (1 + 0.1)\n",
    "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
    "print(sample_size) # 63201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb5d20",
   "metadata": {},
   "source": [
    "### Check for balance in A/B test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the results\n",
    "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique}) \n",
    "print(results)\n",
    "\n",
    "# Find the overall number of unique users using \"len\" and \"unique\"\n",
    "unique_users = len(ab_test_results.uid.unique()) \n",
    "\n",
    "# Find the percentage in each group\n",
    "results = results / unique_users * 100\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique users in each group, by device and gender\n",
    "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
    "\n",
    "# Find the overall number of unique users using \"len\" and \"unique\"\n",
    "unique_users = len(ab_test_results.uid.unique())\n",
    "\n",
    "# Find the percentage in each group\n",
    "results = results / unique_users * 100\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9093eb2",
   "metadata": {},
   "source": [
    "### Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding a p_value\n",
    "def get_pvalue(con_conv, test_conv, con_size, test_size):  \n",
    "    lift =  - abs(test_conv - con_conv)\n",
    "\n",
    "    scale_one = con_conv * (1 - con_conv) * (1 / con_size)\n",
    "    scale_two = test_conv * (1 - test_conv) * (1 / test_size)\n",
    "    scale_val = (scale_one + scale_two)**0.5\n",
    "\n",
    "    p_value = 2 * stats.norm.cdf(lift, loc = 0, scale = scale_val )\n",
    "\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
    "print(p_value) #4.13129774104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.1, test_conv=0.15, con_size=100, test_size=100)\n",
    "print(p_value) #0.283669489407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.48, test_conv=0.50, con_size=1000, test_size=1000)\n",
    "print(p_value) #0.370901935824383"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4c88f",
   "metadata": {},
   "source": [
    "we observed that a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high variance can lead to a high p-value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the p-value\n",
    "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
    "print(p_value)\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value >= 0.05:\n",
    "    print(\"Not Significant\")\n",
    "else:\n",
    "    print(\"Significant Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ed589",
   "metadata": {},
   "source": [
    "### Understanding Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
    "print(confidence_interval) #(0.9755040421682947, 1.0244959578317054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c24137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, 0.95, 2)\n",
    "print(confidence_interval) #(0.6690506448818785, 1.3309493551181215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, 0.95, 0.001)\n",
    "print(confidence_interval) #(1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of our lift distribution \n",
    "lift_mean = test_conv - cont_conv\n",
    "\n",
    "# Calculate variance and standard deviation \n",
    "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv/ cont_size\n",
    "lift_sd = lift_variance**0.5\n",
    "\n",
    "# Find the confidence intervals with cl = 0.95\n",
    "confidence_interval = get_ci(lift_mean, 0.95, lift_sd)\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7efd7",
   "metadata": {},
   "source": [
    "## Interpreting your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70334b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviations\n",
    "control_sd = cont_var**0.5\n",
    "test_sd = test_var**0.5\n",
    "\n",
    "# Create the range of x values \n",
    "control_line = np.linspace(cont_conv - 3 * control_sd, cont_conv + 3 * control_sd, 100)\n",
    "test_line = np.linspace(test_conv - 3 * test_sd, test_conv + 3 * test_sd, 100)\n",
    "\n",
    "# Plot the distribution     \n",
    "plt.plot(control_line, norm.pdf(control_line, cont_conv, control_sd))\n",
    "plt.plot(test_line, norm.pdf(test_line, test_conv, test_sd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lift mean and standard deviation\n",
    "lift_mean = test_conv - cont_conv\n",
    "lift_sd = (test_var + cont_var) ** 0.5\n",
    "\n",
    "# Generate the range of x-values\n",
    "lift_line = np.linspace(lift_mean - 3 * lift_sd, lift_mean + 3 * lift_sd, 100)\n",
    "\n",
    "# Plot the lift distribution\n",
    "plt.plot(lift_line, norm.pdf(lift_line, lift_mean, lift_sd))\n",
    "\n",
    "# Add the annotation lines\n",
    "plt.axvline(x = lift_mean, color = 'green')\n",
    "plt.axvline(x = lwr_ci, color = 'red')\n",
    "plt.axvline(x = upr_ci, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e8bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e38a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f47dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
